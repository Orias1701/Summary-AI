{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import tqdm\n",
    "import json\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_dict = {\n",
    "             'thoi-su': ['chinh-tri', 'dan-sinh', 'lao-dong-viec-lam', 'giao-thong', 'mekong', 'quy-hy-vong'],\n",
    "             'goc-nhin': ['binh-luan-nhieu', 'chinh-tri-chinh-sach', 'y-te-suc-khoe', 'kinh-doanh-quan-tri', 'giao-duc-tri-thuc', 'moi-truong', 'van-hoa-doi-song', 'covid-19', 'tac-gia'],\n",
    "             'the-gioi': ['tu-lieu', 'phan-tich', 'nguoi-viet-5-chau', 'cuoc-song-do-day', 'quan-su'],\n",
    "             'kinh-doanh': ['net-zero', 'quoc-te', 'doanh-nghiep', 'chung-khoan', 'ebank', 'vi-mo', 'tien-cua-toi', 'hang-hoa'],\n",
    "             'bat-dong-san': ['chinh-sach', 'thi-truong', 'khong-gian-song', 'tu-van'],\n",
    "             'khoa-hoc': ['khoa-hoc-trong-nuoc', 'pii-doi-moi-sang-tao', 'tin-tuc', 'phat-minh', 'ung-dung', 'the-gioi-tu-nhien', 'thuong-thuc'],\n",
    "             'giai-tri': ['gioi-sao', 'sach', 'video', 'phim', 'nhac', 'thoi-trang', 'lam-dep', 'san-khau-my-thuat'],\n",
    "             'the-thao': ['bong-da', 'du-lieu-bong-da', 'marathon', 'tennis', 'cac-mon-khac', 'hau-truong'],\n",
    "             'phap-luat': ['ho-so-pha-an', 'tu-van'],\n",
    "             'giao-duc': ['tin-tuc', 'tuyen-sinh', 'chan-dung', 'du-hoc', 'thao-luan', 'hoc-tieng-anh', 'giao-duc-40'],\n",
    "             'suc-khoe': ['tin-tuc', 'cac-benh', 'song-khoe', 'vaccine'],\n",
    "             'doi-song': ['nhip-song', 'to-am', 'bai-hoc-song', 'cooking', 'tieu-dung'],\n",
    "             'du-lich': ['diem-den', 'am-thuc', 'dau-chan', 'tu-van', 'cam-nang'],\n",
    "             'so-hoa': ['cong-nghe', 'san-pham', 'blockchain'],\n",
    "             'xe': ['thi-truong', 'dien-dan'],\n",
    "             'y-kien': ['thoi-su', 'doi-song']\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vietnamese_days = {\n",
    "    \"Chủ nhật\": \"Sunday\",\n",
    "    \"Thứ hai\": \"Monday\",\n",
    "    \"Thứ ba\": \"Tuesday\",\n",
    "    \"Thứ tư\": \"Wednesday\",\n",
    "    \"Thứ năm\": \"Thursday\",\n",
    "    \"Thứ sáu\": \"Friday\",\n",
    "    \"Thứ bảy\": \"Saturday\"\n",
    "}\n",
    "\n",
    "\n",
    "def convert_vietnamese_date(date_str):\n",
    "\n",
    "    for vn_day, en_day in vietnamese_days.items():\n",
    "        if vn_day in date_str:\n",
    "            date_str = date_str.replace(vn_day, en_day)\n",
    "            break\n",
    "\n",
    "    date_str = re.sub(r\"\\s\\(GMT[+-]\\d{1,2}\\)\", \"\", date_str)\n",
    "\n",
    "    return date_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DESCRIPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_url_list = []\n",
    "f_urls = open(\"data/urls/surl_list.jsonl\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "total_count = 0 \n",
    "numlost = 0\n",
    "max_numlost = 3\n",
    "\n",
    "for type in tqdm.tqdm(type_dict, desc=\"Processing types\"):\n",
    "    for sub_type in tqdm.tqdm(type_dict[type], desc=f\"Processing subtypes for {type}\"):\n",
    "        target_count = 33\n",
    "        i = 1\n",
    "        current_count = 0\n",
    "        no_articles_in_a_row = 0\n",
    "        \n",
    "        while current_count < target_count:\n",
    "            url = f\"https://vnexpress.net/{type}/{sub_type}-p{i}\"\n",
    "            content = requests.get(url)\n",
    "\n",
    "            if content.status_code != 200:\n",
    "                print(f\"{content.status_code} Error {type}|{sub_type}\")\n",
    "                break\n",
    "            \n",
    "            soup = BeautifulSoup(content.content, \"html.parser\")\n",
    "            tmp_title_list = soup.find_all(class_=\"title-news\")\n",
    "            \n",
    "            if len(tmp_title_list) == 0:\n",
    "                no_articles_in_a_row += 1\n",
    "                print(f\"No article {type}|{sub_type}.\")\n",
    "                break\n",
    "            \n",
    "            else:\n",
    "                no_articles_in_a_row = 0\n",
    "                for title in tmp_title_list:\n",
    "                    try:\n",
    "                        article_info = title.find_all(\"a\")                                     \n",
    "                        if article_info:\n",
    "                            article_url = article_info[0].get(\"href\")\n",
    "                            article_title = article_info[0].get(\"title\")\n",
    "                            if article_url and article_title:\n",
    "                                description = title.find_next(\"p\", class_=\"description\") \n",
    "                                if description:\n",
    "                                    article_page_content = requests.get(article_url).content\n",
    "                                    article_page_soup = BeautifulSoup(article_page_content, \"html.parser\")\n",
    "                                    article_date = article_page_soup.find(\"span\", class_=\"date\").text.strip() if article_page_soup.find(\"span\", class_=\"date\") else None\n",
    "                                    if article_date:\n",
    "                                        article_date = convert_vietnamese_date(article_date)\n",
    "                                        article_date_obj = datetime.strptime(article_date, \"%A, %d/%m/%Y, %H:%M\")\n",
    "                                        article_year = article_date_obj.year\n",
    "                                        description_text = description.get_text(strip=True)\n",
    "                                        if article_year >= 2020:\n",
    "                                            article_content = \"\"\n",
    "                                            for p_tag in article_page_soup.find_all(\"p\", class_=\"Normal\"):\n",
    "                                                article_content += p_tag.get_text(\" \", strip=True) + \" \" \n",
    "                                            words = article_content.split()\n",
    "                                            word_num = len(words)\n",
    "                                            if word_num > 0 and word_num < 1000:\n",
    "                                                sample = {\n",
    "                                                \"url\": article_url,\n",
    "                                                \"title\": article_title,\n",
    "                                                \"description\": description_text,\n",
    "                                                \"date\": article_date,\n",
    "                                                \"type\": f\"{type}|{sub_type}\",\n",
    "                                                \"words\": word_num,\n",
    "                                                \"content\": article_content,\n",
    "                                                }                                             \n",
    "                                                f_urls.write(json.dumps(sample, indent=4, ensure_ascii=False) + \"\\n\")\n",
    "                                                current_count += 1\n",
    "                                                total_count += 1\n",
    "                                                if current_count % 1 == 0:\n",
    "                                                    print(f\"Total: {total_count}\")\n",
    "                                                if current_count >= target_count:\n",
    "                                                    break\n",
    "                                                numlost = 0\n",
    "                                                \n",
    "                        else:\n",
    "                            numlost += 1\n",
    "                            print(f\"Error {type}|{sub_type}. Continue...\")\n",
    "                            if numlost >= max_numlost:\n",
    "                                print(f\"Error {type}|{sub_type}. Exiting...\")\n",
    "                                break\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error: {e}\")\n",
    "                        pass\n",
    "            \n",
    "            if numlost >= max_numlost:\n",
    "                print(f\"Error {type}|{sub_type}. Exiting....\")\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "        if numlost >= max_numlost:\n",
    "            print(f\"Error {type}|{sub_type}. Exiting.....\")\n",
    "            break\n",
    "\n",
    "f_urls.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

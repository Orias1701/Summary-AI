{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61fd84f4",
   "metadata": {},
   "source": [
    "CRAWL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95c213cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from Libraries.Sorter import ArticleSorter\n",
    "from Libraries.Crawler import CategoryValidator, UrlCollector, ArticleCrawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fd79a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CÁC HÀM XỬ LÝ FILE ===\n",
    "\n",
    "def load_json(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        return []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def replace_json(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def save_json(data, file_path):\n",
    "    with open(file_path, 'a', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def replace_jsonl(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "def save_jsonl(data, file_path):\n",
    "    with open(file_path, 'a', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "\n",
    "# === CÁC HÀM HỖ TRỢ ===\n",
    "\n",
    "def get_urls_from_url_file(file_path):\n",
    "    \"\"\"Lấy set các URL đã có từ file URLS.json.\"\"\"\n",
    "    urls = set()\n",
    "    # Hàm load_json trả về một list các dictionary\n",
    "    url_info_list = load_json(file_path) \n",
    "    for item in url_info_list:\n",
    "        if 'url' in item:\n",
    "            urls.add(item['url'])\n",
    "    return urls\n",
    "\n",
    "def get_existing_article_urls(file_path):\n",
    "    \"\"\"Lấy set các URL bài viết đã có từ file JSONL.\"\"\"\n",
    "    urls = set()\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    urls.add(json.loads(line)['url'])\n",
    "                except (json.JSONDecodeError, KeyError):\n",
    "                    continue\n",
    "    return urls\n",
    "\n",
    "def convert_to_xlsx(json_path, xlsx_path):\n",
    "    \"\"\"Chuyển file JSON (dạng list các object) hoặc JSONL sang XLSX.\"\"\"\n",
    "    try:\n",
    "        # Tự động phát hiện định dạng file\n",
    "        if json_path.endswith('.jsonl'):\n",
    "            df = pd.read_json(json_path, lines=True)\n",
    "        else:\n",
    "            df = pd.read_json(json_path) # Đọc file JSON chuẩn\n",
    "            \n",
    "        column_order = [\"category\", \"sub_category\", \"url\", \"title\", \"description\", \"content\", \"date\", \"words\"]\n",
    "        df = df[[col for col in column_order if col in df.columns]]\n",
    "        df.to_excel(xlsx_path, index=False, engine='openpyxl')\n",
    "        print(f\"-> Đã xuất thành công file Excel tại {xlsx_path}\")\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"-> Không có dữ liệu hoặc lỗi khi chuyển sang Excel: {e}\")\n",
    "\n",
    "def get_url_key(item):\n",
    "    match = re.search(r'-(\\d+)\\.html', item['url'])\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "def heapify(arr, n, i, key_func):\n",
    "    largest = i\n",
    "    l = 2 * i + 1\n",
    "    r = 2 * i + 2\n",
    "    if l < n and key_func(arr[l]) > key_func(arr[largest]): largest = l\n",
    "    if r < n and key_func(arr[r]) > key_func(arr[largest]): largest = r\n",
    "    if largest != i:\n",
    "        arr[i], arr[largest] = arr[largest], arr[i]\n",
    "        heapify(arr, n, largest, key_func)\n",
    "\n",
    "def heapSort(arr, key_func):\n",
    "    n = len(arr)\n",
    "    for i in range(n // 2 - 1, -1, -1):\n",
    "        heapify(arr, n, i, key_func)\n",
    "    for i in range(n - 1, 0, -1):\n",
    "        arr[i], arr[0] = arr[0], arr[i]\n",
    "        heapify(arr, i, 0, key_func)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d84b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_dict = load_json(\"Resource/categories.json\")\n",
    "\n",
    "my_config = {\n",
    "    \"BASE_URL\": \"https://vnexpress.net\",\n",
    "\n",
    "    \"MIN_YEAR\": 2020,\n",
    "    \"MIN_WORDS\": 200,\n",
    "    \"MAX_WORDS\": 1000,\n",
    "    \n",
    "    \"TARGET_ARTICLES_PER_SUBTYPE\": 5, # Số data cần cho mỗi subcategory\n",
    "    \"MAX_CONCURRENT_WORKERS\": 6,\n",
    "    \"VALIDATION_ARTICLES_COUNT\": 5,\n",
    "    \"PROGRESS_TIMEOUT\": 10,\n",
    "    \"ARTICLE_TIMEOUT\": 10,\n",
    "    \"MAX_CONSECUTIVE_FAILURES\": 3,\n",
    "    \"URL_MAX_SUBCATEGORY_FAILURES\": 3, \n",
    "    \"ARTICLE_TIMEOUT\": 5,\n",
    "    \n",
    "    \"TYPE_DICT\": type_dict\n",
    "}\n",
    "\n",
    "resource_dir = \"Resource\"\n",
    "database_dir = \"Database\"\n",
    "pageName = \"VNExpress\"\n",
    "\n",
    "CATE_FILE = f\"{resource_dir}/{pageName}_CATE.json\"\n",
    "DICT_FILE = f\"{resource_dir}/{pageName}_DICT.json\"\n",
    "\n",
    "URLS_PATH = f\"{resource_dir}/{pageName}_URLS\"\n",
    "JSON_PATH = f\"{database_dir}/JSON/{pageName}\"\n",
    "XLSX_PATH = f\"{database_dir}/XLSX/{pageName}\"\n",
    "\n",
    "os.makedirs(resource_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(database_dir, \"JSON\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(database_dir, \"XLSX\"), exist_ok=True)\n",
    "\n",
    "validated_dict = load_json(DICT_FILE)\n",
    "categories_to_crawl = [\n",
    "    'thoi-su', \n",
    "    'the-gioi', \n",
    "    'kinh-doanh', \n",
    "    'bat-dong-san', \n",
    "    'khoa-hoc', \n",
    "    'giai-tri', \n",
    "    'the-thao', \n",
    "    'phap-luat', \n",
    "    'giao-duc', \n",
    "    'suc-khoe', \n",
    "    'doi-song', \n",
    "    'oto-xe-may'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92a35a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Giai đoạn 1: Lấy danh sách chuyên mục hợp lệ ---\n",
    "def getCategories():\n",
    "    validator = CategoryValidator(config=my_config)\n",
    "    valid_categories = validator.run()\n",
    "    replace_json(valid_categories, DICT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8aa277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Giai đoạn 2: Thu thập URL ---\n",
    "def getDict():\n",
    "    for category_name in categories_to_crawl:\n",
    "\n",
    "        URLS_FILE = f\"{URLS_PATH}_{category_name}.json\"\n",
    "        JSON_FILE = f\"{JSON_PATH}_{category_name}.jsonl\"\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"BẮT ĐẦU LẤY URL CHO CATEGORY: {category_name.upper()}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # 1. Lấy danh sách TẤT CẢ các URL đã tồn tại từ cả 2 nguồn\n",
    "        urls_in_url_file = get_urls_from_url_file(URLS_FILE)\n",
    "        urls_in_jsonl_file = get_existing_article_urls(JSON_FILE)\n",
    "        all_existing_urls = urls_in_url_file.union(urls_in_jsonl_file)\n",
    "        \n",
    "        print(f\"Đã tìm thấy {len(all_existing_urls)} URL đã tồn tại cho category này.\")\n",
    "\n",
    "        # 2. Chạy collector để thu thập tất cả URL có thể có\n",
    "        url_collector = UrlCollector(config=my_config)\n",
    "        all_possible_urls = url_collector.run(\n",
    "            valid_subcategories=validated_dict,\n",
    "            categories_to_process=[category_name]\n",
    "        )\n",
    "\n",
    "        # 3. Lọc ra chỉ những URL thực sự mới\n",
    "        new_urls_info = [\n",
    "            info for info in all_possible_urls \n",
    "            if info['url'] not in all_existing_urls\n",
    "        ]\n",
    "        \n",
    "        print(f\"Thu thập được {len(new_urls_info)} URL mới.\")\n",
    "\n",
    "        # 4. Ghi lại file URLS với dữ liệu cũ + mới\n",
    "        if new_urls_info:\n",
    "            existing_urls_info = load_json(URLS_FILE)\n",
    "            combined_urls_info = existing_urls_info + new_urls_info\n",
    "            replace_json(combined_urls_info, URLS_FILE)\n",
    "    \n",
    "# === END ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98165855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Giai đoạn 3: Crawl nội dung bài viết ---\n",
    "def runCrawl():\n",
    "    for category_name in categories_to_crawl:\n",
    "\n",
    "        URLS_FILE = f\"{URLS_PATH}_{category_name}.json\"\n",
    "        JSON_FILE = f\"{JSON_PATH}_{category_name}.jsonl\"\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"BẮT ĐẦU CRAWL CHO CATEGORY: {category_name.upper()}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        urls_for_category = load_json(URLS_FILE)\n",
    "        if not urls_for_category:\n",
    "            print(\"Không có URL nào trong file để crawl. Bỏ qua.\")\n",
    "            continue\n",
    "\n",
    "        # 1. Xác định các URL đã được xử lý từ trước\n",
    "        existing_urls_in_jsonl = get_existing_article_urls(JSON_FILE)\n",
    "        print(f\"Tìm thấy {len(existing_urls_in_jsonl)} bài viết đã tồn tại trong file {JSON_FILE}.\")\n",
    "        \n",
    "        # 2. Chạy crawler\n",
    "        article_crawler = ArticleCrawler(config=my_config)\n",
    "        new_articles, successfully_crawled_urls = article_crawler.run(\n",
    "            urls_to_crawl=urls_for_category,\n",
    "            category=category_name,\n",
    "            existing_article_urls=existing_urls_in_jsonl\n",
    "        )\n",
    "        \n",
    "        # 3. Lưu dữ liệu mới nếu có\n",
    "        if new_articles:\n",
    "            save_jsonl(new_articles, JSON_FILE)\n",
    "            print(f\"Đã lưu {len(new_articles)} bài báo mới vào {JSON_FILE}.\")\n",
    "\n",
    "        # 4. Xóa các URL đã được xử lý khỏi file URLS_FILE\n",
    "        processed_urls = existing_urls_in_jsonl.union(set(successfully_crawled_urls))\n",
    "        if processed_urls:\n",
    "            # Lọc ra danh sách các URL còn lại\n",
    "            remaining_urls = [\n",
    "                info for info in urls_for_category \n",
    "                if info['url'] not in processed_urls\n",
    "            ]\n",
    "            \n",
    "            # Ghi đè lại file URLS_FILE\n",
    "            replace_json(remaining_urls, URLS_FILE)\n",
    "            print(f\"Đã xóa {len(processed_urls)} URL đã xử lý. Còn lại {len(remaining_urls)} URL trong hàng đợi.\")\n",
    "\n",
    "# === END ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f217d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Giai đoạn 4: Sort bài viết ---\n",
    "def sortCrawl():\n",
    "    for category_name in categories_to_crawl:\n",
    "\n",
    "        URLS_FILE = f\"{URLS_PATH}_{category_name}.json\"\n",
    "        JSON_FILE = f\"{JSON_PATH}_{category_name}.jsonl\"\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"BẮT ĐẦU SORT CHO CATEGORY: {category_name.upper()}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # --- Sắp xếp URLS_FILE ---\n",
    "        urls_data = load_json(URLS_FILE)\n",
    "        if urls_data:\n",
    "            sorted_urls = heapSort(urls_data, key_func=get_url_key)\n",
    "            replace_json(sorted_urls, URLS_FILE)\n",
    "\n",
    "        # --- Sắp xếp JSON_FILE ---\n",
    "        all_articles = load_jsonl(JSON_FILE)\n",
    "        if all_articles:\n",
    "            sorter = ArticleSorter(categories_file_path=CATE_FILE)\n",
    "            sorted_articles = sorter.sort_and_deduplicate(all_articles)\n",
    "            replace_jsonl(sorted_articles, JSON_FILE)\n",
    "        \n",
    "        print(f\"-> Đã hoàn tất sắp xếp cho category: {category_name}\")\n",
    "\n",
    "# === END ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94490648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Giai đoạn 5: Tổng hợp và Hoàn thiện Dữ liệu (Logic cuối cùng, tự động) ---\n",
    "def finalizeData():\n",
    "    # Định nghĩa đường dẫn cho các file master\n",
    "    MASTER_JSONL_FILE = f\"{JSON_PATH}.jsonl\"\n",
    "    MASTER_JSON_FILE = f\"{JSON_PATH}.json\" \n",
    "    MASTER_XLSX_FILE = f\"{XLSX_PATH}.xlsx\"\n",
    "\n",
    "    categories_dict = load_json(CATE_FILE)\n",
    "    if not categories_dict:\n",
    "        return\n",
    "    \n",
    "    all_categories = list(categories_dict.keys())\n",
    "\n",
    "    # Xóa các file tổng hợp cũ để bắt đầu lại từ đầu\n",
    "    for f in [MASTER_JSONL_FILE, MASTER_JSON_FILE]:\n",
    "        if os.path.exists(f):\n",
    "            os.remove(f)\n",
    "\n",
    "    # 1. Gộp dữ liệu từ các file JSONL của tất cả category tìm được\n",
    "    print(\"\\n--- Bước 1: Gộp dữ liệu từ các file category ---\")\n",
    "    for category_name in all_categories: # Sử dụng danh sách vừa đọc được\n",
    "        JSON_FILE = f\"{JSON_PATH}_{category_name}.jsonl\"\n",
    "        # Chỉ xử lý nếu file của category đó tồn tại\n",
    "        if os.path.exists(JSON_FILE):\n",
    "            articles_data = load_jsonl(JSON_FILE)\n",
    "            if articles_data:\n",
    "                save_jsonl(articles_data, MASTER_JSONL_FILE)\n",
    "\n",
    "    # 2. Sắp xếp và xóa trùng lặp file tổng hợp\n",
    "    print(\"\\n--- Bước 2: Sắp xếp và dọn dẹp file tổng hợp ---\")\n",
    "    all_merged_articles = load_jsonl(MASTER_JSONL_FILE)\n",
    "    if all_merged_articles:\n",
    "        sorter = ArticleSorter(categories_file_path=CATE_FILE)\n",
    "        final_sorted_articles = sorter.sort_and_deduplicate(all_merged_articles)\n",
    "\n",
    "        # 3. Ghi kết quả ra cả 2 định dạng file\n",
    "        print(\"\\n--- Bước 3: Ghi file tổng hợp ở cả 2 định dạng (.jsonl và .json) ---\")\n",
    "        replace_jsonl(final_sorted_articles, MASTER_JSONL_FILE)\n",
    "        replace_json(final_sorted_articles, MASTER_JSON_FILE)\n",
    "\n",
    "        # 4. Xuất file Excel cuối cùng\n",
    "        print(\"\\n--- Bước 4: Xuất file Excel tổng hợp ---\")\n",
    "        convert_to_xlsx(MASTER_JSON_FILE, MASTER_XLSX_FILE)\n",
    "\n",
    "# === END ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7b9ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getCategories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a9f75ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d263c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# runCrawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af18dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sortCrawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b04d776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finalizeData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27fd100",
   "metadata": {},
   "source": [
    "TRAIN and TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa840550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from Libraries import Trainer\n",
    "\n",
    "training_config = {\n",
    "    # --- Đường dẫn và tên model ---\n",
    "    \"DATA_JSONL_FILE\": f\"{JSON_PATH}.jsonl\",\n",
    "    \"MODEL_CHECKPOINT\": \"vinai/bartpho-syllable\",\n",
    "    \"OUTPUT_MODEL_DIR\": \"Models/bartpho-summarizer\",\n",
    "    \n",
    "    # --- Hyperparameters ---\n",
    "    \"MAX_INPUT_LENGTH\": 1024,\n",
    "    \"MAX_TARGET_LENGTH\": 256,\n",
    "    \"BATCH_SIZE\": 4,\n",
    "    \"NUM_TRAIN_EPOCHS\": 3,\n",
    "    \"LEARNING_RATE\": 3e-5,\n",
    "    \"WEIGHT_DECAY\": 0.01,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca5c4c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRAIN ===\n",
    "def modelTrain():\n",
    "    summarizer_trainer = Trainer.SummarizationTrainer(config=training_config)\n",
    "    summarizer_trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ba83bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TEST ===\n",
    "def modelTest():\n",
    "    fine_tuned_model_path = training_config[\"OUTPUT_MODEL_DIR\"] \n",
    "    summarizer_pipeline = pipeline(\"summarization\", model=fine_tuned_model_path)\n",
    "\n",
    "    # Lấy một bài báo từ dữ liệu của bạn để tóm tắt thử\n",
    "    df = pd.read_json(training_config[\"DATA_JSONL_FILE\"], lines=True)\n",
    "    sample_text = df.iloc[400][\"content\"] # Số 50\n",
    "\n",
    "    print(\"--- VĂN BẢN GỐC ---\")\n",
    "    print(sample_text)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "    print(\"--- BẢN TÓM TẮT TỪ MODEL ---\")\n",
    "    summary = summarizer_pipeline(sample_text, max_length=256, min_length=50, do_sample=False)\n",
    "    print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ced07ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e59220b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- VĂN BẢN GỐC ---\n",
      "Thông tin trên được ông Vũ Bá Phú, Cục trưởng Cục Xúc tiến thương mại cùng các doanh nghiệp và hiệp hội ngành nghề, thông tin trong tọa đàm: \"Thực trạng và thách thức trong xuất khẩu hàng sang Mỹ\" sáng 8/8 tại TP HCM. Ông Phú cho rằng Mỹ từ lâu được xem như \"mỏ vàng\" của thương mại toàn cầu, với sức mua trải rộng từ áo quần giá 1 USD đến vài trăm USD mỗi chiếc, hay trái cây từ loại phổ thông đến cao cấp. Theo ông, dù thuế nhập khẩu vào Mỹ tăng lên 20% hay 40%, hàng Việt vẫn phải tìm đường vào, do đó, làm thế nào để hàng xuất đi có lợi nhất. Với quy định mới, hàng xuất xứ Việt Nam sẽ chịu thuế 20%, trong khi hàng \"mượn xuất xứ\" phải gánh 40%. Sáu tháng đầu năm, xuất khẩu sang Mỹ tăng mạnh, kéo theo lượng tồn kho lớn, kịch bản từng xảy ra sau Covid-19. Số liệu Cục Thống kê cho thấy 7 tháng qua, xuất siêu sang Mỹ đạt gần 75 tỷ USD, tăng 28,6% so với cùng kỳ năm ngoái. Tuy nhiên, ông Phú dự báo từ nửa cuối 2025, thậm chí sang nửa đầu 2026, nhiều ngành khó duy trì tốc độ tăng trưởng 28% như thời gian qua. Diễn biến này cũng được ghi nhận ở dệt may - ngành xuất khẩu chủ lực. Ông Trần Như Tùng, Chủ tịch HĐQT Dệt may Đầu tư Thương mại Thành Công (TCM), kiêm Phó Chủ tịch VITAS, cho biết doanh nghiệp đã \"chạy đơn\" trước khi Mỹ áp thuế đối ứng khiến kim ngạch 6 tháng đầu năm sang Mỹ tăng hơn 17%. Nhưng chính đợt đẩy hàng này có thể khiến quý III hụt đơn, do tồn kho lớn và khách chờ chính sách rõ ràng. Ông Tùng kỳ vọng nếu thuế ổn định ở 20%, đơn hàng sẽ quay lại từ quý IV, song nhấn mạnh ngành vẫn đối mặt bốn thách thức: tiêu chuẩn \"xanh\" làm tăng chi phí, cạnh tranh gay gắt, quy tắc xuất xứ phụ thuộc nguyên liệu Trung Quốc và áp lực đầu tư số hóa với chi phí ERP tới 2 triệu USD. Ngành gỗ cũng ở thế \"vừa thuận vừa khó\". Thuế 20% vẫn là lợi thế so với Trung Quốc (50-55%) hay Ấn Độ (50%), nhưng mức 40% với hàng trung chuyển tiềm ẩn rủi ro lớn do nguyên liệu nhập từ Trung Quốc còn nhiều. Ông Điền Quang Hiệp, Chủ tịch Hiệp hội Chế biến gỗ Bình Dương (BIFA), đề nghị có hướng dẫn xuất xứ rõ ràng để tránh vướng mắc và khuyến nghị định giá hợp lý từ đầu, thay vì bị ép giảm sâu khi thương lượng với khách Mỹ. Về nông sản, ngoài sức ép chịu thuế 20% với hàng sang Mỹ, doanh nghiệp phải gánh chi phí nội địa tăng do thuế 5% với nông dân và thủ tục hoàn VAT phức tạp. Ông Nguyễn Mạnh Hùng, Chủ tịch Nafoods Group, cho biết thương lái Trung Quốc mua trực tiếp tại vùng nguyên liệu nhưng không đóng thuế minh bạch, gây bất lợi lớn cho doanh nghiệp Việt. Giải pháp, theo ông, là giảm phụ thuộc nguyên liệu nhập, xây dựng thương hiệu, mở nhà máy tại Mỹ và châu Âu, đồng thời đơn giản hóa thủ tục cấp CO để tách bạch mức thuế 20% và 40%. Trước bối cảnh đó, ông Phú cho rằng chiến lược dài hạn phải là phát triển công nghiệp hỗ trợ để giảm phụ thuộc vào linh kiện và nguyên liệu nhập khẩu, gia tăng tỷ lệ nội địa hóa và nâng khả năng đáp ứng tiêu chuẩn quốc tế về môi trường, lao động, xuất xứ. Riêng nông sản cần chuyển mạnh sang chế biến sâu, siết chặt chứng từ và minh bạch hồ sơ xuất khẩu. Ông cũng đánh giá cao vai trò của các nền tảng B2B như VietnamUSA.Arobid.com - giải pháp vừa ra mắt trong bối cảnh doanh nghiệp Việt đối mặt mức thuế mới. Nền tảng này giúp kết nối trực tiếp với khách hàng Mỹ thông qua số hóa giao thương, minh bạch nguồn gốc và đáp ứng tiêu chuẩn ESG. Chủ tịch Arobid, ông Trần Văn Chín, kỳ vọng đây sẽ trở thành \"trạm\" kết nối toàn diện, hỗ trợ doanh nghiệp vượt qua ba rào cản then chốt: số hóa, vốn và ESG - nền tảng cho xuất khẩu bền vững sang Mỹ. Thi Hà\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- BẢN TÓM TẮT TỪ MODEL ---\n",
      "Nhiều doanh nghiệp cho biết dù thuế 20%, hàng Việt vẫn phải tìm đường vào Mỹ, do đó cần có chiến lược dài hạn để giảm phụ thuộc vào nguyên liệu nhập khẩu. (Ảnh minh hoạ). Theo Cục Xúc tiến thương mại, dù thuế 20%, hàng Việt vẫn phải tìm đường vào Mỹ.\n"
     ]
    }
   ],
   "source": [
    "modelTest()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

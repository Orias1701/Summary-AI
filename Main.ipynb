{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61fd84f4",
   "metadata": {},
   "source": [
    "IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c213cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from Libraries import Trainer\n",
    "from transformers import pipeline\n",
    "from Libraries import Processor as pc\n",
    "from Libraries.Sorter import ArticleSorter\n",
    "# from Libraries.Crawler import CategoryValidator, UrlCollector, ArticleCrawler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e5986b",
   "metadata": {},
   "source": [
    "CRAWL CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d84b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_dict = pc.load_json(\"Resource/categories.json\")\n",
    "\n",
    "my_config = {\n",
    "    \"BASE_URL\": \"https://vnexpress.net\",\n",
    "\n",
    "    \"MIN_YEAR\": 2020,\n",
    "    \"MIN_WORDS\": 200,\n",
    "    \"MAX_WORDS\": 1000,\n",
    "    \n",
    "    \"TARGET_ARTICLES_PER_SUBTYPE\": 5,\n",
    "    \"MAX_CONCURRENT_WORKERS\": 6,\n",
    "    \"VALIDATION_ARTICLES_COUNT\": 5,\n",
    "    \"PROGRESS_TIMEOUT\": 10,\n",
    "    \"ARTICLE_TIMEOUT\": 10,\n",
    "    \"MAX_CONSECUTIVE_FAILURES\": 3,\n",
    "    \"URL_MAX_SUBCATEGORY_FAILURES\": 3, \n",
    "    \"ARTICLE_TIMEOUT\": 5,\n",
    "    \n",
    "    \"TYPE_DICT\": type_dict\n",
    "}\n",
    "\n",
    "resource_dir = \"Resource\"\n",
    "database_dir = \"Database\"\n",
    "pageName = \"VNExpress\"\n",
    "\n",
    "CATE_FILE = f\"{resource_dir}/{pageName}_CATE.json\"\n",
    "DICT_FILE = f\"{resource_dir}/{pageName}_DICT.json\"\n",
    "\n",
    "URLS_PATH = f\"{resource_dir}/{pageName}_URLS\"\n",
    "JSON_PATH = f\"{database_dir}/JSON/{pageName}\"\n",
    "XLSX_PATH = f\"{database_dir}/XLSX/{pageName}\"\n",
    "\n",
    "os.makedirs(resource_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(database_dir, \"JSON\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(database_dir, \"XLSX\"), exist_ok=True)\n",
    "\n",
    "validated_dict = pc.load_json(DICT_FILE)\n",
    "categories_to_crawl = [\n",
    "    'thoi-su', \n",
    "    'the-gioi', \n",
    "    'kinh-doanh', \n",
    "    'bat-dong-san', \n",
    "    'khoa-hoc', \n",
    "    'giai-tri', \n",
    "    'the-thao', \n",
    "    'phap-luat', \n",
    "    'giao-duc', \n",
    "    'suc-khoe', \n",
    "    'doi-song', \n",
    "    'oto-xe-may'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b63713",
   "metadata": {},
   "source": [
    "CRAWL FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a35a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Giai đoạn 1: Lấy danh sách chuyên mục hợp lệ ---\n",
    "def getCategories():\n",
    "    validator = CategoryValidator(config=my_config)\n",
    "    valid_categories = validator.run()\n",
    "    pc.replace_json(valid_categories, DICT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aa277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Giai đoạn 2: Thu thập URL ---\n",
    "def getDict():\n",
    "    for category_name in categories_to_crawl:\n",
    "\n",
    "        URLS_FILE = f\"{URLS_PATH}_{category_name}.json\"\n",
    "        JSON_FILE = f\"{JSON_PATH}_{category_name}.jsonl\"\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"BẮT ĐẦU LẤY URL CHO CATEGORY: {category_name.upper()}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # 1. Lấy danh sách TẤT CẢ các URL đã tồn tại từ cả 2 nguồn\n",
    "        urls_in_url_file = pc.get_urls_from_url_file(URLS_FILE)\n",
    "        urls_in_jsonl_file = pc.get_existing_article_urls(JSON_FILE)\n",
    "        all_existing_urls = urls_in_url_file.union(urls_in_jsonl_file)\n",
    "        \n",
    "        print(f\"Đã tìm thấy {len(all_existing_urls)} URL đã tồn tại cho category này.\")\n",
    "\n",
    "        # 2. Chạy collector để thu thập tất cả URL có thể có\n",
    "        url_collector = UrlCollector(config=my_config)\n",
    "        all_possible_urls = url_collector.run(\n",
    "            valid_subcategories=validated_dict,\n",
    "            categories_to_process=[category_name]\n",
    "        )\n",
    "\n",
    "        # 3. Lọc ra chỉ những URL thực sự mới\n",
    "        new_urls_info = [\n",
    "            info for info in all_possible_urls \n",
    "            if info['url'] not in all_existing_urls\n",
    "        ]\n",
    "        \n",
    "        print(f\"Thu thập được {len(new_urls_info)} URL mới.\")\n",
    "\n",
    "        # 4. Ghi lại file URLS với dữ liệu cũ + mới\n",
    "        if new_urls_info:\n",
    "            existing_urls_info = pc.load_json(URLS_FILE)\n",
    "            combined_urls_info = existing_urls_info + new_urls_info\n",
    "            pc.replace_json(combined_urls_info, URLS_FILE)\n",
    "    \n",
    "# === END ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98165855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Giai đoạn 3: Crawl nội dung bài viết ---\n",
    "def runCrawl():\n",
    "    for category_name in categories_to_crawl:\n",
    "\n",
    "        URLS_FILE = f\"{URLS_PATH}_{category_name}.json\"\n",
    "        JSON_FILE = f\"{JSON_PATH}_{category_name}.jsonl\"\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"BẮT ĐẦU CRAWL CHO CATEGORY: {category_name.upper()}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        urls_for_category = pc.load_json(URLS_FILE)\n",
    "        if not urls_for_category:\n",
    "            print(\"Không có URL nào trong file để crawl. Bỏ qua.\")\n",
    "            continue\n",
    "\n",
    "        # 1. Xác định các URL đã được xử lý từ trước\n",
    "        existing_urls_in_jsonl = pc.get_existing_article_urls(JSON_FILE)\n",
    "        print(f\"Tìm thấy {len(existing_urls_in_jsonl)} bài viết đã tồn tại trong file {JSON_FILE}.\")\n",
    "        \n",
    "        # 2. Chạy crawler\n",
    "        article_crawler = ArticleCrawler(config=my_config)\n",
    "        new_articles, successfully_crawled_urls = article_crawler.run(\n",
    "            urls_to_crawl=urls_for_category,\n",
    "            category=category_name,\n",
    "            existing_article_urls=existing_urls_in_jsonl\n",
    "        )\n",
    "        \n",
    "        # 3. Lưu dữ liệu mới nếu có\n",
    "        if new_articles:\n",
    "            pc.save_jsonl(new_articles, JSON_FILE)\n",
    "            print(f\"Đã lưu {len(new_articles)} bài báo mới vào {JSON_FILE}.\")\n",
    "\n",
    "        # 4. Xóa các URL đã được xử lý khỏi file URLS_FILE\n",
    "        processed_urls = existing_urls_in_jsonl.union(set(successfully_crawled_urls))\n",
    "        if processed_urls:\n",
    "            # Lọc ra danh sách các URL còn lại\n",
    "            remaining_urls = [\n",
    "                info for info in urls_for_category \n",
    "                if info['url'] not in processed_urls\n",
    "            ]\n",
    "            \n",
    "            # Ghi đè lại file URLS_FILE\n",
    "            pc.replace_json(remaining_urls, URLS_FILE)\n",
    "            print(f\"Đã xóa {len(processed_urls)} URL đã xử lý. Còn lại {len(remaining_urls)} URL trong hàng đợi.\")\n",
    "\n",
    "# === END ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef74cc8",
   "metadata": {},
   "source": [
    "RESIGN DATA FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f217d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Giai đoạn 4: Sort bài viết ---\n",
    "def sortCrawl():\n",
    "    for category_name in categories_to_crawl:\n",
    "\n",
    "        URLS_FILE = f\"{URLS_PATH}_{category_name}.json\"\n",
    "        JSON_FILE = f\"{JSON_PATH}_{category_name}.jsonl\"\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"BẮT ĐẦU SORT CHO CATEGORY: {category_name.upper()}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # --- Sắp xếp URLS_FILE ---\n",
    "        urls_data = pc.load_json(URLS_FILE)\n",
    "        if urls_data:\n",
    "            sorted_urls = pc.heapSort(urls_data, key_func=pc.get_url_key)\n",
    "            pc.replace_json(sorted_urls, URLS_FILE)\n",
    "\n",
    "        # --- Sắp xếp JSON_FILE ---\n",
    "        all_articles = pc.load_jsonl(JSON_FILE)\n",
    "        if all_articles:\n",
    "            sorter = ArticleSorter(categories_file_path=CATE_FILE)\n",
    "            sorted_articles = sorter.sort_and_deduplicate(all_articles)\n",
    "            pc.replace_jsonl(sorted_articles, JSON_FILE)\n",
    "        \n",
    "        print(f\"-> Đã hoàn tất sắp xếp cho category: {category_name}\")\n",
    "\n",
    "# === END ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94490648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Giai đoạn 5: Tổng hợp và Hoàn thiện Dữ liệu (Logic cuối cùng, tự động) ---\n",
    "def finalizeData():\n",
    "    # Định nghĩa đường dẫn cho các file master\n",
    "    MASTER_JSONL_FILE = f\"{JSON_PATH}.jsonl\"\n",
    "    MASTER_JSON_FILE = f\"{JSON_PATH}.json\" \n",
    "    MASTER_XLSX_FILE = f\"{XLSX_PATH}.xlsx\"\n",
    "\n",
    "    categories_dict = pc.load_json(CATE_FILE)\n",
    "    if not categories_dict:\n",
    "        return\n",
    "    \n",
    "    all_categories = list(categories_dict.keys())\n",
    "\n",
    "    # Xóa các file tổng hợp cũ để bắt đầu lại từ đầu\n",
    "    for f in [MASTER_JSONL_FILE, MASTER_JSON_FILE]:\n",
    "        if os.path.exists(f):\n",
    "            os.remove(f)\n",
    "\n",
    "    # 1. Gộp dữ liệu từ các file JSONL của tất cả category tìm được\n",
    "    print(\"\\n--- Bước 1: Gộp dữ liệu từ các file category ---\")\n",
    "    for category_name in all_categories: # Sử dụng danh sách vừa đọc được\n",
    "        JSON_FILE = f\"{JSON_PATH}_{category_name}.jsonl\"\n",
    "        # Chỉ xử lý nếu file của category đó tồn tại\n",
    "        if os.path.exists(JSON_FILE):\n",
    "            articles_data = pc.load_jsonl(JSON_FILE)\n",
    "            if articles_data:\n",
    "                pc.save_jsonl(articles_data, MASTER_JSONL_FILE)\n",
    "\n",
    "    # 2. Sắp xếp và xóa trùng lặp file tổng hợp\n",
    "    print(\"\\n--- Bước 2: Sắp xếp và dọn dẹp file tổng hợp ---\")\n",
    "    all_merged_articles = pc.load_jsonl(MASTER_JSONL_FILE)\n",
    "    if all_merged_articles:\n",
    "        sorter = ArticleSorter(categories_file_path=CATE_FILE)\n",
    "        final_sorted_articles = sorter.sort_and_deduplicate(all_merged_articles)\n",
    "\n",
    "        # 3. Ghi kết quả ra cả 2 định dạng file\n",
    "        print(\"\\n--- Bước 3: Ghi file tổng hợp ở cả 2 định dạng (.jsonl và .json) ---\")\n",
    "        pc.replace_jsonl(final_sorted_articles, MASTER_JSONL_FILE)\n",
    "        pc.replace_json(final_sorted_articles, MASTER_JSON_FILE)\n",
    "\n",
    "        # 4. Xuất file Excel cuối cùng\n",
    "        print(\"\\n--- Bước 4: Xuất file Excel tổng hợp ---\")\n",
    "        pc.convert_to_xlsx(MASTER_JSON_FILE, MASTER_XLSX_FILE)\n",
    "\n",
    "# === END ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e3d91",
   "metadata": {},
   "source": [
    "TRAINING CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa840550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "JSON_PATH = f\"{database_dir}/Data/TrainData\"\n",
    "\n",
    "training_config = {\n",
    "    # --- Đường dẫn và tên model ---\n",
    "    \"DATA_JSONL_FILE\": f\"{JSON_PATH}.jsonl\",\n",
    "    \"MODEL_CHECKPOINT\": \"vinai/bartpho-syllable\",\n",
    "    \"OUTPUT_MODEL_DIR\": \"Models/bartpho-summarizer\",\n",
    "    \n",
    "    # --- Hyperparameters ---\n",
    "    \"MAX_INPUT_LENGTH\": 1024,\n",
    "    \"MAX_TARGET_LENGTH\": 256,\n",
    "    \"BATCH_SIZE\": 4,\n",
    "    \"NUM_TRAIN_EPOCHS\": 3,\n",
    "    \"LEARNING_RATE\": 3e-5,\n",
    "    \"WEIGHT_DECAY\": 0.01,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe94646",
   "metadata": {},
   "source": [
    "TRAINING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5c4c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRAIN ===\n",
    "def modelTrain():\n",
    "    summarizer_trainer = Trainer.SummarizationTrainer(config=training_config)\n",
    "    summarizer_trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba83bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TEST ===\n",
    "def modelTest():\n",
    "    fine_tuned_model_path = training_config[\"OUTPUT_MODEL_DIR\"] \n",
    "    summarizer_pipeline = pipeline(\"summarization\", model=fine_tuned_model_path)\n",
    "\n",
    "    # Lấy một bài báo từ dữ liệu để tóm tắt thử\n",
    "    df = pd.read_json(training_config[\"DATA_JSONL_FILE\"], lines=True)\n",
    "    sample_text = df.iloc[50][\"article\"]\n",
    "    # sample_text = df.iloc[50][\"content\"]\n",
    "\n",
    "    print(\"--- VĂN BẢN GỐC ---\")\n",
    "    print(sample_text)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "    print(\"--- BẢN TÓM TẮT TỪ MODEL ---\")\n",
    "    summary = summarizer_pipeline(\n",
    "        sample_text,\n",
    "        max_length=training_config[\"MAX_TARGET_LENGTH\"],\n",
    "        min_length=50,\n",
    "        do_sample=False,\n",
    "        num_beams=4,\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de46a87",
   "metadata": {},
   "source": [
    "RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b9ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getCategories()\n",
    "# getDict()\n",
    "# runCrawl()\n",
    "# sortCrawl()\n",
    "# finalizeData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ced07ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTrain()\n",
    "modelTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c084f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main.ipynb\n",
    "\n",
    "# !python Libraries/API.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

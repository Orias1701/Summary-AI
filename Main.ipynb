{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61fd84f4",
   "metadata": {},
   "source": [
    "CRAWL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95c213cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "from Libraries.Crawler import CategoryValidator, UrlCollector, ArticleCrawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fd79a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CÁC HÀM XỬ LÝ FILE ===\n",
    "\n",
    "def load_json(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        return []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def replace_json(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def save_json(data, file_path):\n",
    "    with open(file_path, 'a', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def replace_jsonl(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "def save_jsonl(data, file_path):\n",
    "    with open(file_path, 'a', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# === CÁC HÀM HỖ TRỢ ===\n",
    "\n",
    "def get_existing_article_urls(file_path):\n",
    "    \"\"\"Lấy set các URL bài viết đã có từ file JSONL.\"\"\"\n",
    "    urls = set()\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    urls.add(json.loads(line)['url'])\n",
    "                except (json.JSONDecodeError, KeyError):\n",
    "                    continue\n",
    "    return urls\n",
    "\n",
    "def convert_to_xlsx(jsonl_path, xlsx_path):\n",
    "    \"\"\"Chuyển file JSONL sang XLSX.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_json(jsonl_path, lines=True)\n",
    "        column_order = [\"category\", \"sub_category\", \"url\", \"title\", \"description\", \"content\", \"date\", \"words\"]\n",
    "        df = df[[col for col in column_order if col in df.columns]]\n",
    "        df.to_excel(xlsx_path, index=False, engine='openpyxl')\n",
    "    except (FileNotFoundError, ValueError):\n",
    "        pass\n",
    "\n",
    "def get_url_key(item):\n",
    "    match = re.search(r'-(\\d+)\\.html', item['url'])\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "def heapify(arr, n, i, key_func):\n",
    "    largest = i\n",
    "    l = 2 * i + 1\n",
    "    r = 2 * i + 2\n",
    "    if l < n and key_func(arr[l]) > key_func(arr[largest]): largest = l\n",
    "    if r < n and key_func(arr[r]) > key_func(arr[largest]): largest = r\n",
    "    if largest != i:\n",
    "        arr[i], arr[largest] = arr[largest], arr[i]\n",
    "        heapify(arr, n, largest, key_func)\n",
    "\n",
    "def heapSort(arr, key_func):\n",
    "    n = len(arr)\n",
    "    for i in range(n // 2 - 1, -1, -1):\n",
    "        heapify(arr, n, i, key_func)\n",
    "    for i in range(n - 1, 0, -1):\n",
    "        arr[i], arr[0] = arr[0], arr[i]\n",
    "        heapify(arr, i, 0, key_func)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d84b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_dict = load_json(\"Resource/categories.json\")\n",
    "\n",
    "my_config = {\n",
    "    \"BASE_URL\": \"https://vnexpress.net\",\n",
    "\n",
    "    \"MIN_YEAR\": 2020,\n",
    "    \"MIN_WORDS\": 200,\n",
    "    \"MAX_WORDS\": 1000,\n",
    "    \n",
    "    \"TARGET_ARTICLES_PER_SUBTYPE\": 30,\n",
    "    \"MAX_CONCURRENT_WORKERS\": 6,\n",
    "    \"VALIDATION_ARTICLES_COUNT\": 5,\n",
    "    \"PROGRESS_TIMEOUT\": 10,\n",
    "    \"ARTICLE_TIMEOUT\": 10,\n",
    "    \"MAX_CONSECUTIVE_FAILURES\": 3,\n",
    "    \"URL_MAX_SUBCATEGORY_FAILURES\": 3, \n",
    "    \"ARTICLE_TIMEOUT\": 5,\n",
    "    \n",
    "    \"TYPE_DICT\": type_dict\n",
    "}\n",
    "\n",
    "resource_dir = \"Resource\"\n",
    "database_dir = \"Database\"\n",
    "pageName = \"VNExpress\"\n",
    "\n",
    "CATE_FILE = f\"{resource_dir}/{pageName}_CATE.json\"\n",
    "DICT_FILE = f\"{resource_dir}/{pageName}_DICT.json\"\n",
    "URLS_FILE = f\"{resource_dir}/{pageName}_URLS.json\"\n",
    "JSON_FILE = f\"{database_dir}/JSON/{pageName}.jsonl\"\n",
    "XLSX_FILE = f\"{database_dir}/XLSX/{pageName}.xlsx\"\n",
    "TEST_FILE = f\"{resource_dir}/test.json\"\n",
    "\n",
    "os.makedirs(resource_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(database_dir, \"JSON\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(database_dir, \"XLSX\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92a35a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Giai đoạn 1: Lấy danh sách chuyên mục hợp lệ ---\n",
    "def getCategories():\n",
    "    validator = CategoryValidator(config=my_config)\n",
    "    valid_categories = validator.run()\n",
    "    replace_json(valid_categories, DICT_FILE)\n",
    "\n",
    "# getCategories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e455663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Giai đoạn 2: Thu thập URL ---\n",
    "def getURLDict():\n",
    "    validated_dict = load_json(DICT_FILE)\n",
    "    if validated_dict:\n",
    "        url_collector = UrlCollector(config=my_config)\n",
    "        all_urls = url_collector.run(valid_subcategories=validated_dict)\n",
    "        save_json(all_urls, URLS_FILE)\n",
    "    else:\n",
    "        print(\"Không có chuyên mục hợp lệ nào, dừng quy trình.\")\n",
    "\n",
    "# getURLDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37c6041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Giai đoạn 3: Crawl nội dung bài viết ---\n",
    "def finalCrawl():\n",
    "    urlindex = 1\n",
    "    URLS_FILE = f\"{resource_dir}/{pageName}_URLS_{urlindex}.json\"\n",
    "\n",
    "    urls_to_crawl = load_json(URLS_FILE)\n",
    "    if urls_to_crawl:\n",
    "        print(f\"\\nĐang sắp xếp {len(urls_to_crawl)} URLs bằng Heapsort...\")\n",
    "        sorted_urls = heapSort(urls_to_crawl, key_func=get_url_key)\n",
    "        print(\"Sắp xếp hoàn tất.\")\n",
    "        existing_urls = get_existing_article_urls(JSON_FILE)\n",
    "        \n",
    "        article_crawler = ArticleCrawler(config=my_config)\n",
    "        new_articles, crawled_urls_list = article_crawler.run(\n",
    "            urls_to_crawl=sorted_urls, \n",
    "            existing_article_urls=existing_urls\n",
    "        )\n",
    "        \n",
    "        if new_articles:\n",
    "            save_jsonl(new_articles, JSON_FILE)\n",
    "            \n",
    "        print(f\"\\nDanh sách các URL đã crawl thành công ({len(crawled_urls_list)}):\")\n",
    "    else:\n",
    "        print(\"Không có URL nào để crawl.\")\n",
    "\n",
    "# finalCrawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fdf60ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu gốc có 966 bài báo.\n",
      "Bắt đầu sắp xếp 966 bài báo...\n",
      "Sắp xếp hoàn tất.\n"
     ]
    }
   ],
   "source": [
    "from Libraries.Sorter import ArticleSorter\n",
    "all_articles = load_jsonl(JSON_FILE)\n",
    "sorter = ArticleSorter(categories_file_path=CATE_FILE)\n",
    "sorted_articles = sorter.sort_and_deduplicate(all_articles)\n",
    "replace_jsonl(sorted_articles, JSON_FILE)\n",
    "convert_to_xlsx(JSON_FILE, XLSX_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27fd100",
   "metadata": {},
   "source": [
    "TRAIN and TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa840550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from Libraries import Trainer\n",
    "\n",
    "training_config = {\n",
    "    # --- Đường dẫn và tên model ---\n",
    "    \"DATA_JSONL_FILE\": \"Database/JSON/vnexpress_articles.jsonl\",\n",
    "    \"MODEL_CHECKPOINT\": \"vinai/bartpho-syllable\",\n",
    "    \"OUTPUT_MODEL_DIR\": \"Models/bartpho-summarizer\",\n",
    "    \n",
    "    # --- Hyperparameters ---\n",
    "    \"MAX_INPUT_LENGTH\": 1024,\n",
    "    \"MAX_TARGET_LENGTH\": 256,\n",
    "    \"BATCH_SIZE\": 4,\n",
    "    \"NUM_TRAIN_EPOCHS\": 3,\n",
    "    \"LEARNING_RATE\": 3e-5,\n",
    "    \"WEIGHT_DECAY\": 0.01,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca5c4c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRAIN ===\n",
    "summarizer_trainer = Trainer.SummarizationTrainer(config=training_config)\n",
    "# summarizer_trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ba83bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TEST ===\n",
    "def modelTest():\n",
    "    fine_tuned_model_path = training_config[\"OUTPUT_MODEL_DIR\"] \n",
    "    summarizer_pipeline = pipeline(\"summarization\", model=fine_tuned_model_path)\n",
    "\n",
    "    # Lấy một bài báo từ dữ liệu của bạn để tóm tắt thử\n",
    "    df = pd.read_json(training_config[\"DATA_JSONL_FILE\"], lines=True)\n",
    "    sample_text = df.iloc[50][\"content\"] # Số 50\n",
    "\n",
    "    print(\"--- VĂN BẢN GỐC ---\")\n",
    "    print(sample_text)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "    print(\"--- BẢN TÓM TẮT TỪ MODEL ---\")\n",
    "    summary = summarizer_pipeline(sample_text, max_length=256, min_length=50, do_sample=False)\n",
    "    print(summary[0]['summary_text'])\n",
    "\n",
    "# modelTest()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

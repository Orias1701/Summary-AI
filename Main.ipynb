{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61fd84f4",
   "metadata": {},
   "source": [
    "CRAWL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95c213cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from Libraries.Sorter import ArticleSorter\n",
    "from Libraries.Crawler import CategoryValidator, UrlCollector, ArticleCrawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fd79a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CÁC HÀM XỬ LÝ FILE ===\n",
    "\n",
    "def load_json(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        return []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def replace_json(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def save_json(data, file_path):\n",
    "    with open(file_path, 'a', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def replace_jsonl(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "def save_jsonl(data, file_path):\n",
    "    with open(file_path, 'a', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "\n",
    "# === CÁC HÀM HỖ TRỢ ===\n",
    "\n",
    "def get_urls_from_url_file(file_path):\n",
    "    \"\"\"Lấy set các URL đã có từ file URLS.json.\"\"\"\n",
    "    urls = set()\n",
    "    # Hàm load_json trả về một list các dictionary\n",
    "    url_info_list = load_json(file_path) \n",
    "    for item in url_info_list:\n",
    "        if 'url' in item:\n",
    "            urls.add(item['url'])\n",
    "    return urls\n",
    "\n",
    "def get_existing_article_urls(file_path):\n",
    "    \"\"\"Lấy set các URL bài viết đã có từ file JSONL.\"\"\"\n",
    "    urls = set()\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    urls.add(json.loads(line)['url'])\n",
    "                except (json.JSONDecodeError, KeyError):\n",
    "                    continue\n",
    "    return urls\n",
    "\n",
    "def convert_to_xlsx(jsonl_path, xlsx_path):\n",
    "    \"\"\"Chuyển file JSONL sang XLSX.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_json(jsonl_path, lines=True)\n",
    "        column_order = [\"category\", \"sub_category\", \"url\", \"title\", \"description\", \"content\", \"date\", \"words\"]\n",
    "        df = df[[col for col in column_order if col in df.columns]]\n",
    "        df.to_excel(xlsx_path, index=False, engine='openpyxl')\n",
    "    except (FileNotFoundError, ValueError):\n",
    "        pass\n",
    "\n",
    "def get_url_key(item):\n",
    "    match = re.search(r'-(\\d+)\\.html', item['url'])\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "def heapify(arr, n, i, key_func):\n",
    "    largest = i\n",
    "    l = 2 * i + 1\n",
    "    r = 2 * i + 2\n",
    "    if l < n and key_func(arr[l]) > key_func(arr[largest]): largest = l\n",
    "    if r < n and key_func(arr[r]) > key_func(arr[largest]): largest = r\n",
    "    if largest != i:\n",
    "        arr[i], arr[largest] = arr[largest], arr[i]\n",
    "        heapify(arr, n, largest, key_func)\n",
    "\n",
    "def heapSort(arr, key_func):\n",
    "    n = len(arr)\n",
    "    for i in range(n // 2 - 1, -1, -1):\n",
    "        heapify(arr, n, i, key_func)\n",
    "    for i in range(n - 1, 0, -1):\n",
    "        arr[i], arr[0] = arr[0], arr[i]\n",
    "        heapify(arr, i, 0, key_func)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d84b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_dict = load_json(\"Resource/categories.json\")\n",
    "\n",
    "my_config = {\n",
    "    \"BASE_URL\": \"https://vnexpress.net\",\n",
    "\n",
    "    \"MIN_YEAR\": 2020,\n",
    "    \"MIN_WORDS\": 200,\n",
    "    \"MAX_WORDS\": 1000,\n",
    "    \n",
    "    \"TARGET_ARTICLES_PER_SUBTYPE\": 5, #Số data cần cho mỗi subcategory\n",
    "    \"MAX_CONCURRENT_WORKERS\": 6,\n",
    "    \"VALIDATION_ARTICLES_COUNT\": 5,\n",
    "    \"PROGRESS_TIMEOUT\": 10,\n",
    "    \"ARTICLE_TIMEOUT\": 10,\n",
    "    \"MAX_CONSECUTIVE_FAILURES\": 3,\n",
    "    \"URL_MAX_SUBCATEGORY_FAILURES\": 3, \n",
    "    \"ARTICLE_TIMEOUT\": 5,\n",
    "    \n",
    "    \"TYPE_DICT\": type_dict\n",
    "}\n",
    "\n",
    "resource_dir = \"Resource\"\n",
    "database_dir = \"Database\"\n",
    "pageName = \"VNExpress\"\n",
    "\n",
    "CATE_FILE = f\"{resource_dir}/{pageName}_CATE.json\"\n",
    "DICT_FILE = f\"{resource_dir}/{pageName}_DICT.json\"\n",
    "\n",
    "URLS_PATH = f\"{resource_dir}/{pageName}_URLS\"\n",
    "JSON_PATH = f\"{database_dir}/JSON/{pageName}\"\n",
    "XLSX_PATH = f\"{database_dir}/XLSX/{pageName}\"\n",
    "\n",
    "os.makedirs(resource_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(database_dir, \"JSON\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(database_dir, \"XLSX\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a35a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Giai đoạn 1: Lấy danh sách chuyên mục hợp lệ ---\n",
    "def getCategories():\n",
    "    validator = CategoryValidator(config=my_config)\n",
    "    valid_categories = validator.run()\n",
    "    replace_json(valid_categories, DICT_FILE)\n",
    "\n",
    "# getCategories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aa277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_to_crawl = ['thoi-su', 'the-gioi', 'kinh-doanh']\n",
    "validated_dict = load_json(DICT_FILE)\n",
    "\n",
    "# --- Giai đoạn 2: Thu thập URL ---\n",
    "def getDict():\n",
    "    for category_name in categories_to_crawl:\n",
    "\n",
    "        URLS_FILE = f\"{URLS_PATH}_{category_name}.json\"\n",
    "        JSON_FILE = f\"{JSON_PATH}_{category_name}.jsonl\"\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"BẮT ĐẦU LẤY URL CHO CATEGORY: {category_name.upper()}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # 1. Lấy danh sách TẤT CẢ các URL đã tồn tại từ cả 2 nguồn\n",
    "        urls_in_url_file = get_urls_from_url_file(URLS_FILE)\n",
    "        urls_in_jsonl_file = get_existing_article_urls(JSON_FILE)\n",
    "        all_existing_urls = urls_in_url_file.union(urls_in_jsonl_file)\n",
    "        \n",
    "        print(f\"Đã tìm thấy {len(all_existing_urls)} URL đã tồn tại cho category này.\")\n",
    "\n",
    "        # 2. Chạy collector để thu thập tất cả URL có thể có\n",
    "        url_collector = UrlCollector(config=my_config)\n",
    "        all_possible_urls = url_collector.run(\n",
    "            valid_subcategories=validated_dict,\n",
    "            categories_to_process=[category_name]\n",
    "        )\n",
    "\n",
    "        # 3. Lọc ra chỉ những URL thực sự mới\n",
    "        new_urls_info = [\n",
    "            info for info in all_possible_urls \n",
    "            if info['url'] not in all_existing_urls\n",
    "        ]\n",
    "        \n",
    "        print(f\"Thu thập được {len(new_urls_info)} URL mới.\")\n",
    "\n",
    "        # 4. Ghi lại file URLS với dữ liệu cũ + mới\n",
    "        if new_urls_info:\n",
    "            existing_urls_info = load_json(URLS_FILE)\n",
    "            combined_urls_info = existing_urls_info + new_urls_info\n",
    "            replace_json(combined_urls_info, URLS_FILE)\n",
    "    \n",
    "\n",
    "# --- Giai đoạn 3: Crawl nội dung bài viết ---\n",
    "def runCrawl():\n",
    "    for category_name in categories_to_crawl:\n",
    "\n",
    "        URLS_FILE = f\"{URLS_PATH}_{category_name}.json\"\n",
    "        JSON_FILE = f\"{JSON_PATH}_{category_name}.jsonl\"\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"BẮT ĐẦU CRAWL CHO CATEGORY: {category_name.upper()}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        urls_for_category = load_json(URLS_FILE)\n",
    "\n",
    "        if urls_for_category:\n",
    "            existing_urls = get_existing_article_urls(JSON_FILE)        \n",
    "            article_crawler = ArticleCrawler(config=my_config)\n",
    "            new_articles = article_crawler.run(\n",
    "                urls_to_crawl=urls_for_category,\n",
    "                category=category_name,\n",
    "                existing_article_urls=existing_urls\n",
    "            )\n",
    "            \n",
    "            if new_articles:\n",
    "                save_jsonl(new_articles, JSON_FILE)\n",
    "\n",
    "\n",
    "# --- Giai đoạn 4: Sort bài viết ---\n",
    "def sortCrawl():\n",
    "    for category_name in categories_to_crawl:\n",
    "\n",
    "        URLS_FILE = f\"{URLS_PATH}_{category_name}.json\"\n",
    "        JSON_FILE = f\"{JSON_PATH}_{category_name}.jsonl\"\n",
    "        XLSX_FILE = f\"{XLSX_PATH}_{category_name}.xlsx\"\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"BẮT ĐẦU SORT CHO CATEGORY: {category_name.upper()}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # --- BƯỚC 1: SẮP XẾP URLS_FILE ---\n",
    "        print(f\"Đang xử lý file URL: {URLS_FILE}...\")\n",
    "        urls_data = load_json(URLS_FILE)\n",
    "        if urls_data:\n",
    "            sorted_urls = heapSort(urls_data, key_func=get_url_key)\n",
    "            replace_json(sorted_urls, URLS_FILE)\n",
    "            print(f\"-> Đã sắp xếp và cập nhật {URLS_FILE}.\")\n",
    "        else:\n",
    "            print(f\"-> Không có dữ liệu trong {URLS_FILE} để sắp xếp.\")\n",
    "\n",
    "        # --- BƯỚC 2: SẮP XẾP JSON_FILE ---\n",
    "        print(f\"\\nĐang xử lý file bài viết: {JSON_FILE}...\")\n",
    "        all_articles = load_jsonl(JSON_FILE)\n",
    "        if all_articles:\n",
    "            sorter = ArticleSorter(categories_file_path=CATE_FILE)\n",
    "            sorted_articles = sorter.sort_and_deduplicate(all_articles)\n",
    "            replace_jsonl(sorted_articles, JSON_FILE)\n",
    "            print(f\"-> Đã sắp xếp và cập nhật {JSON_FILE}.\")\n",
    "            convert_to_xlsx(JSON_FILE, XLSX_FILE)\n",
    "        else:\n",
    "             print(f\"-> Không có dữ liệu trong {JSON_FILE} để sắp xếp.\")\n",
    "    \n",
    "# === END ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9f75ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "getDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d263c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "runCrawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af18dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortCrawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27fd100",
   "metadata": {},
   "source": [
    "TRAIN and TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa840550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from Libraries import Trainer\n",
    "\n",
    "training_config = {\n",
    "    # --- Đường dẫn và tên model ---\n",
    "    \"DATA_JSONL_FILE\": \"Database/JSON/vnexpress_articles.jsonl\",\n",
    "    \"MODEL_CHECKPOINT\": \"vinai/bartpho-syllable\",\n",
    "    \"OUTPUT_MODEL_DIR\": \"Models/bartpho-summarizer\",\n",
    "    \n",
    "    # --- Hyperparameters ---\n",
    "    \"MAX_INPUT_LENGTH\": 1024,\n",
    "    \"MAX_TARGET_LENGTH\": 256,\n",
    "    \"BATCH_SIZE\": 4,\n",
    "    \"NUM_TRAIN_EPOCHS\": 3,\n",
    "    \"LEARNING_RATE\": 3e-5,\n",
    "    \"WEIGHT_DECAY\": 0.01,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5c4c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRAIN ===\n",
    "summarizer_trainer = Trainer.SummarizationTrainer(config=training_config)\n",
    "# summarizer_trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba83bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TEST ===\n",
    "def modelTest():\n",
    "    fine_tuned_model_path = training_config[\"OUTPUT_MODEL_DIR\"] \n",
    "    summarizer_pipeline = pipeline(\"summarization\", model=fine_tuned_model_path)\n",
    "\n",
    "    # Lấy một bài báo từ dữ liệu của bạn để tóm tắt thử\n",
    "    df = pd.read_json(training_config[\"DATA_JSONL_FILE\"], lines=True)\n",
    "    sample_text = df.iloc[50][\"content\"] # Số 50\n",
    "\n",
    "    print(\"--- VĂN BẢN GỐC ---\")\n",
    "    print(sample_text)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "    print(\"--- BẢN TÓM TẮT TỪ MODEL ---\")\n",
    "    summary = summarizer_pipeline(sample_text, max_length=256, min_length=50, do_sample=False)\n",
    "    print(summary[0]['summary_text'])\n",
    "\n",
    "# modelTest()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
